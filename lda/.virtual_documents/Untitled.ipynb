





#python implementation of LDA
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix



#read the data and assign meaningfull column names
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
cls = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
dataset = pd.read_csv(url, names=cls)


# Divide the data set into features (X) and target variable (y)
# Encode the labels using LabelEncoder() -> Map categorical values into numeric: does not assume inherit order 
X = dataset.iloc[:, 0:4].values
y = dataset.iloc[:, 4].values
# encode
le = LabelEncoder()
y = le.fit_transform(y)


# Exploratory Data Analysis
# Create a pair plot to visualize relationships between different features and species
ax = sns.pairplot(dataset, hue='Class', markers=["o", "s", "D"])
plt.suptitle("Pair Plot of Iris Dataset")
sns.move_legend(
    ax, "lower center",
    bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False)
plt.tight_layout()
plt.show()





# Visualizing the distribution of each feature using histograms
plt.figure(figsize=(12, 6))
for i, feature in enumerate(cls[:-1]):
    plt.subplot(2, 2, i + 1)
    sns.histplot(data=dataset, x=feature, hue="Class", kde=True)
    plt.title(f"{feature} Distribution")





# Correlation heatmaps
correlation_matrix  = dataset.corr(numeric_only=True)
plt.figure(figsize=(8,6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()





# Split the data set into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)


# Transform the four-dimensional feature space into a two-dimensional subspace while maximizing the separation between classes. The outcome is a training data set with reduced dimensions.
lda = LinearDiscriminantAnalysis(n_components=2)
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)


# Visualize the reduced dimensional data
tmp_Df = pd.DataFrame(X_train, columns=["LDA Component 1", "LDA Component 2"])
tmp_Df["Class"] = y_train
sns.FacetGrid(tmp_Df, hue="Class",
             height = 6).map(plt.scatter,
                            "LDA Component 1",
                            "LDA Component 2")


# Train the dimensional data on a random forest
classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)


# Evaluate the LDA Model
accuracy = accuracy_score(y_test, y_pred)
accuracy



